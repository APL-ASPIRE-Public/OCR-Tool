{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run on TensorFlow 2.x\n",
    "%tensorflow_version 2.x\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import relevant modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# The following lines adjust the granularity of reporting. \n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "# The following line improves formatting when ouputting NumPy arrays.\n",
    "np.set_printoptions(linewidth = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(1):\n",
    "  plt.subplot(3,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(x_train[i], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Digit: {}\".format(y_train[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(x_train[0], cmap='gray', interpolation='none')\n",
    "plt.title(\"Digit: {}\".format(y_train[0]))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.subplot(2,1,2)\n",
    "plt.hist(x_train[0].reshape(784))\n",
    "plt.title(\"Pixel Value Distribution\")\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's print the shape before we reshape and normalize\n",
    "print(\"X_train shape\", x_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"X_test shape\", x_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)\n",
    "\n",
    "# building the input vector from the 28x28 pixels\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# normalizing the data to help with the training\n",
    "x_train_normalized = x_train / 255.0\n",
    "x_test_normalized = x_test / 255.0\n",
    "\n",
    "# print the final input shape ready for training\n",
    "print( x_train.shape)\n",
    "print( x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(y_train, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define the plotting function\n",
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "  # list_of_metrics should be one of the names shown in:\n",
    "  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Value\")\n",
    "\n",
    "  for m in list_of_metrics:\n",
    "    x = hist[m]\n",
    "    plt.plot(epochs[1:], x[1:], label=m)\n",
    "\n",
    "  plt.legend()\n",
    "\n",
    "print(\"Loaded the plot_curve function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(my_learning_rate):\n",
    "  \"\"\"Create and compile a deep neural net.\"\"\"\n",
    "  \n",
    "  # All models in this course are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # The features are stored in a two-dimensional 28X28 array. \n",
    "  # Flatten that two-dimensional array into a a one-dimensional \n",
    "  # 784-element array.\n",
    "  model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "  # Define the first hidden layer.   \n",
    "  model.add(tf.keras.layers.Dense(units=32, activation='swish'))\n",
    "  \n",
    "  # Define a dropout regularization layer. \n",
    "  model.add(tf.keras.layers.Dropout(rate=0.2))\n",
    "\n",
    "  # Define the output layer. The units parameter is set to 10 because\n",
    "  # the model must choose among 10 possible output values (representing\n",
    "  # the digits from 0 to 9, inclusive).\n",
    "  #\n",
    "  # Don't change this layer.\n",
    "  model.add(tf.keras.layers.Dense(units=10, activation='softmax'))     \n",
    "                           \n",
    "  # Construct the layers into a model that TensorFlow can execute.  \n",
    "  # Notice that the loss function for multi-class classification\n",
    "  # is different than the loss function for binary classification.  \n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=my_learning_rate),\n",
    "                loss=\"sparse_categorical_crossentropy\",\n",
    "                metrics=['accuracy'])\n",
    "  \n",
    "  return model    \n",
    "\n",
    "\n",
    "def train_model(model, train_features, train_label, epochs,\n",
    "                batch_size=None, validation_split=0.1):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  history = model.fit(x=train_features, y=train_label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True, \n",
    "                      validation_split=validation_split)\n",
    " \n",
    "  # To track the progression of training, gather a snapshot\n",
    "  # of the model's metrics at each epoch. \n",
    "  epochs = history.epoch\n",
    "  hist = pd.DataFrame(history.history)\n",
    "\n",
    "  return epochs, hist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.003\n",
    "epochs = 50\n",
    "batch_size = 4000\n",
    "validation_split = 0.2\n",
    "\n",
    "my_model = create_model(learning_rate)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "epochs, hist = train_model(my_model, x_train_normalized, y_train, \n",
    "                           epochs, batch_size, validation_split)\n",
    "\n",
    "list_of_metrics_to_plot = ['accuracy']\n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
    "\n",
    "# Evaluate against the test set.\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x=x_test_normalized, y=y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}