# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GXAtzaPVJp3FZiUJGQ0PBpSQjPfFSU18
"""

#install.packages("keras")
library(keras)

mnist <- dataset_mnist() # Load the MNIST dataset
train_images <- mnist$train$x # create a 60,000x28x28 tensor for the training images
train_labels <- mnist$train$y # create a 60,000-element vector for the training labels
test_images <- mnist$test$x   # create a 10,000x28x28 tensor for the test images
test_labels <- mnist$test$y   # create a 10,000-element vector for the test labels

str(train_images)

str(train_labels)

digit <- train_images[200,,]      # select the 200th training image
plot(as.raster(digit, max = 255)) # plot it!

network <- keras_model_sequential() %>%  # create a linear stack of layers
  layer_dense(units = 784, activation = "relu", input_shape = c(28 * 28)) %>%  # input layer
  layer_dense(units = 512, activation = "relu") %>%                            # hidden layer
  layer_dense(units = 10,  activation = "softmax")                            # output layer

network %>% compile(
  optimizer = "rmsprop",             # network will update itself based on the training data & loss
  loss = "categorical_crossentropy", # measure mismatch between y_pred and y, calculated after each minibatch
  metrics = c("accuracy")            # measure of performace - correctly classified images
)

train_images <- array_reshape(train_images, c(60000, 28 * 28)) # Unfold each 28x28 image into a linear vector of 784. Train_images was an image cube, now it's a 2d matrix where each row is an image.
train_images <- train_images / 255  # Normalize each element in the matrix to [0,1]

test_images <- array_reshape(test_images, c(10000, 28 * 28)) # Do the same for the test images
test_images <- test_images / 255

dim(train_images) # Check to see that the dimensions are now correct

# The dimensions of our y labels need to match the dimensions of our output layer
train_labels <- to_categorical(train_labels) # makes key-value boolean dummy vars out of numerical vectors
test_labels <- to_categorical(test_labels)   # do the same with the test labels

dim(train_labels)

dim(test_labels)
## [1] 10000    10

history <- network %>% fit(train_images, train_labels, epochs = 20, batch_size = 1000, validation_split = 0.1)

plot(history)

network %>% predict_classes(test_images[1:10,]) # display the predicted category for the first ten test images

mnist$test$y[1:10] # display the actual labels for the first ten test images (subtract one from the array index to get the predicted value)

network %>% evaluate(test_images, test_labels)

network2 <- keras_model_sequential()
network2 %>% 
  layer_dense(units = 512, activation = "relu", input_shape = c(28*28)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 512, activation = "relu") %>%  # new layer
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 10, activation = "softmax")

network2 %>% compile(
  optimizer = "rmsprop", 
  loss = "categorical_crossentropy", 
  metrics = c("accuracy"))

history <- network2 %>% fit(train_images, train_labels, 
                            epochs = 20, batch_size = 1000, 
                            validation_split = 0.1)

network2 %>% evaluate(test_images, test_labels) # run the model on the full test set